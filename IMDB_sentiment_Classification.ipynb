{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LL6J0xtg3ams"
   },
   "outputs": [],
   "source": [
    "# The IMDB sentiment classification dataset consists of 50,000 movie reviews from IMDB users that are labeled as either positive (1) or negative (0). \n",
    "# The reviews are preprocessed and each one is encoded as a sequence of word indexes in the form of integers.\n",
    "# The words within the reviews are indexed by their overall frequency within the dataset. For example, the integer “2” encodes the second most frequent word in the data.\n",
    "# The 50,000 reviews are split into 25,000 for training and 25,000 for testing.\n",
    "# Text Process word by word at diffrent timestamp ( You may use RNN(Recurrent Neural Network) LSTM(Long Short-Term Memory) GRU(Gated Recurrent Unit))\n",
    "# convert input text to vector reprent input text\n",
    "# DOMAIN: Digital content and entertainment industry\n",
    "# CONTEXT: The objective of this project is to build a text classification model that analyses the customer's sentiments based on their reviews in the IMDB database. The model uses a complex deep learning model to build an embedding layer followed by a classification algorithm to analyse the sentiment of the customers.\n",
    "# DATA DESCRIPTION: The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). \n",
    "# Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
    "# For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word.\n",
    "# Use the first 20 words from each review to speed up training, using a max vocabulary size of 10,000. \n",
    "# As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "# PROJECT OBJECTIVE: Build a sequential NLP classifier which can use input text parameters to determine the customer sentiments.\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "idlchnJq6zUF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'import' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#loading imdb data with most frequent 10000 words\u001b[39;00m\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m imdb\n\u001b[0;32m      4\u001b[0m (X_train, y_train), (X_test, y_test) \u001b[38;5;241m=\u001b[39m imdb\u001b[38;5;241m.\u001b[39mload_data(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m) \u001b[38;5;66;03m# you may take top 10,000 word frequently review of movies\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#consolidating data for EDA(exploratory data analysis: involves gathering all the relevant data into one place and preparing it for analysis )\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "#loading imdb data with most frequent 10000 words\n",
    "from keras.datasets import imdb\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000) # you may take top 10,000 word frequently review of movies\n",
    "#consolidating data for EDA(exploratory data analysis: involves gathering all the relevant data into one place and preparing it for analysis )\n",
    "data = np.concatenate((X_train, X_test), axis=0)\n",
    "label = np.concatenate((y_train, y_test), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b9fxt_He-8ML",
    "outputId": "3b3ec207-d487-419e-cfc3-06fbac2d6d30"
   },
   "outputs": [],
   "source": [
    "print(\"Review is \",X_train[5]) # series of no converted word to vocabulory associated with index\n",
    "print(\"Review is \",y_train[5]) # 0 indicating a negative review and 1 indicating a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZFeXON4_kpS",
    "outputId": "d87936f3-eb2d-44ca-b844-c0d6cba2666d"
   },
   "outputs": [],
   "source": [
    "vocab=imdb.get_word_index() #The code you provided retrieves the word index for the IMDB dataset\n",
    "print(vocab) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q65wLcxoQKr8",
    "outputId": "9d1e98bb-2093-4776-f5b6-a28dfbd70516"
   },
   "outputs": [],
   "source": [
    "data #data is a numpy array that contains all the text data from the IMDB dataset, both the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MRtMVfW_QNQQ",
    "outputId": "4e541714-862c-4d8b-ed19-a4048892e75e"
   },
   "outputs": [],
   "source": [
    "label #label is a numpy array that contains all the sentiment labels from the IMDB dataset, both the training and testing sets.0 indicates a negative review and 1 indicates a positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHyV6TYn7Eyd",
    "outputId": "42d2e638-3940-4d51-b638-eeb4601be75f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y3RVAgfP6789",
    "outputId": "b9eb7bd7-7508-4ed6-8c35-389293868483"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_test\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3bXE5bL1QtD4",
    "outputId": "d9daec87-c028-4a24-b603-6425ae66d0c7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_train\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_train' is not defined"
     ]
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZH3ijDRXQuk1",
    "outputId": "85ac8d17-8b90-4677-a924-529ce8e71478"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y_test\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "y_test # y_test is 25000, which indicates that it contains 25000 sentiment labels, one for each review in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P_Z7n5pe6_-U"
   },
   "outputs": [],
   "source": [
    "# Function to perform relevant sequence adding on the data\n",
    "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it contains exactly 1000 numbers. \n",
    "# That means we fill every review that is shorter than 500 with zeros. \n",
    "# We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. \n",
    "# We also transform the targets into floats.\n",
    "# sequences is name of method the review less than 1000 we perform padding overthere\n",
    "\n",
    "def vectorize(sequences, dimension = 10000):\n",
    "  # Create an all-zero matrix of shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results\n",
    "\n",
    "# The script transforms your dataset into a binary vector space model.\n",
    "# First, if we examine the x_train content we see that each review is represented as a sequence of word ids. Each word id corresponds to one specific word:\n",
    "# print(train_data[0])  # print the first review\n",
    "# [1, 14, 22, 16, 43, 530, 973, ..., 5345, 19, 178, 32]\n",
    "# the size of the entire dictionary, dictionary=10000 in your example. \n",
    "# We will then associate each element/index of this vector with one word/word_id. \n",
    "# So word represented by word id 14 will now be represented by 14-th element of this vector.\n",
    "\n",
    "\n",
    "# Each element will either be 0 (word is not present in the review) or 1 (word is present in the review). \n",
    "# And we can treat this as a probability, so we even have meaning for values in between 0 and 1. \n",
    "# Furthermore, every review will now be represented by this very long (sparse) vector which has a constant length for every review.\n",
    "# word      word_id\n",
    "# I      -> 0\n",
    "# you    -> 1\n",
    "# he     -> 2\n",
    "# be     -> 3\n",
    "# eat    -> 4\n",
    "# happy  -> 5\n",
    "# sad    -> 6\n",
    "# banana -> 7\n",
    "# a      -> 8\n",
    "\n",
    "# the sentences would then be processed in a following way.\n",
    "\n",
    "\n",
    "# I be happy      -> [0,3,5]   -> [1,0,0,1,0,1,0,0,0]\n",
    "# I eat a banana. -> [0,4,8,7] -> [1,0,0,0,1,0,0,1,1]\n",
    "\n",
    "# Now I highlighted the word sparse. \n",
    "# That means, there will have A LOT MORE zeros in comparison with ones. \n",
    "# We can take advantage of that. Instead of checking every word, whether it is contained in a review or not; \n",
    "# we will check a substantially smaller list of only those words that DO appear in our review.\n",
    "\n",
    "# Therefore, we can make things easy for us and create reviews × vocabulary matrix of zeros right away by np.zeros((len(sequences), dimension))\n",
    "# And then just go through words in each review and flip the indicator to 1.0 at position corresponding to that word:\n",
    "\n",
    "# result[review_id][word_id] = 1.0\n",
    "# So instead of doing 25000 x 10000 = 250 000 000 operations, \n",
    "# we only did number of words = 5 967 841. That's just ~2.5% of original amount of operations.\n",
    "\n",
    "\n",
    "# The for loop here is not processing all the matrix. As you can see, it enumerates elements of the sequence, so it's looping only on one dimension. Let's take a simple example :\n",
    "# t = np.array([1,2,3,4,5,6,7,8,9])\n",
    "# r = np.zeros((len(t), 10))\n",
    "\n",
    "#Output\n",
    "\n",
    "# array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#  [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "# [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) #\n",
    "\n",
    "# then we modify elements with the same way you have :\n",
    "\n",
    "# for i, s in enumerate(t):\n",
    "#  r[i,s] = 1.\n",
    "# array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
    "#   [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])\n",
    "  # you can see that the for loop modified only a set of elements (len(t))\n",
    "  # which has index [i,s] (in this case ; (0, 1), (1, 2), (2, 3), an so on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6laDpGAeJjJ7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Now we split our data into a training and a testing set. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# The training set will contain  reviews and the testing set \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m test_x \u001b[38;5;241m=\u001b[39m data[:\u001b[38;5;241m10000\u001b[39m]\n\u001b[0;32m      5\u001b[0m test_y \u001b[38;5;241m=\u001b[39m label[:\u001b[38;5;241m10000\u001b[39m]\n\u001b[0;32m      6\u001b[0m train_x \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m10000\u001b[39m:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Now we split our data into a training and a testing set. \n",
    "# The training set will contain  reviews and the testing set \n",
    "\n",
    "test_x = data[:10000]\n",
    "test_y = label[:10000]\n",
    "train_x = data[10000:]\n",
    "train_y = label[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hOJcTUYjJn0x",
    "outputId": "1a3b0f76-5a3d-415a-8c71-c2b75f514566"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_x\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_x' is not defined"
     ]
    }
   ],
   "source": [
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7UUjUI1YJrPe",
    "outputId": "4fc320f0-b68a-45dc-dbc1-cd7c49b71dc9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_y' is not defined"
     ]
    }
   ],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vhIxyrxWJtj_",
    "outputId": "9559eef4-87d6-418f-a890-bba4401c6a44"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_x\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cgl5dFbDJt2h",
    "outputId": "50f09e17-5360-4865-b537-302d7f757975"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_y\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_y' is not defined"
     ]
    }
   ],
   "source": [
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zMpdzIkW-848",
    "outputId": "a3950525-6d77-4fd6-9039-1308ec10d1b8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCategories:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39munique(label))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of unique words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(np\u001b[38;5;241m.\u001b[39mhstack(data))))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Categories:\", np.unique(label))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
    "# The hstack() function is used to stack arrays in sequence horizontally (column wise).\n",
    "\n",
    "#>>> import numpy as np\n",
    "#>>> x = np.array((3,5,7))\n",
    "#>>> y = np.array((5,7,9))\n",
    "#>>> np.hstack((x,y))\n",
    "# array([3, 5, 7, 5, 7, 9])\n",
    "\n",
    "\n",
    "# You can see in the output above that the dataset is labeled into two categories, either 0 or 1, \n",
    "# which represents the sentiment of the review. \n",
    "\n",
    "# The whole dataset contains 9998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e2sd-WK_YyL",
    "outputId": "9f22332f-2d58-456c-ceec-3bccfbd8c442"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m length \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Review length:\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(length))\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStandard Deviation:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(np\u001b[38;5;241m.\u001b[39mstd(length)))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "length = [len(i) for i in data]\n",
    "print(\"Average Review length:\", np.mean(length))\n",
    "print(\"Standard Deviation:\", round(np.std(length)))\n",
    "\n",
    "# The whole dataset contains 9998 unique words and the average review length is 234 words, with a standard deviation of 173 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGIeB-bD_dDZ",
    "outputId": "50894b9b-d403-413e-c8a8-195939b1dc2c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# If you look at the data you will realize it has been already pre-processed.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# All words have been mapped to integers and the integers represent the words sorted by their frequency.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# This is very common in text analysis to represent a dataset like this. \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Let's look at a single training example:\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel:\u001b[39m\u001b[38;5;124m\"\u001b[39m, label[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "# If you look at the data you will realize it has been already pre-processed.\n",
    "# All words have been mapped to integers and the integers represent the words sorted by their frequency.\n",
    "# This is very common in text analysis to represent a dataset like this. \n",
    "# So 4 represents the 4th most used word, \n",
    "# 5 the 5th most used word and so on... \n",
    "# The integer 1 is reserved for the start marker,\n",
    "# the integer 2 for an unknown word and 0 for padding.\n",
    "\n",
    "# Let's look at a single training example:\n",
    "\n",
    "print(\"Label:\", label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tx2_Wbqo_g3d",
    "outputId": "e482a4e4-2fbc-4b7d-e095-c89b95668ee4"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "print(data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rq38Xbjy_jlv",
    "outputId": "5a0f5a5d-660f-4672-d31e-e66d6fef0485"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imdb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Let's decode the first review\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Above you see the first review of the dataset which is labeled as positive (1).\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Retrieves a dict mapping words to their index in the IMDB dataset.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m index \u001b[38;5;241m=\u001b[39m imdb\u001b[38;5;241m.\u001b[39mget_word_index()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# If there is a possibility of multiple keys with the same value, you will need to specify the desired behaviour in this case to lookup more than 2 values\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# ivd = dict((v, k) for k, v in d.items())\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# If you want to peek at the reviews yourself and see what people have actually written, you can reverse the process too:\u001b[39;00m\n\u001b[0;32m     12\u001b[0m reverse_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m([(value, key) \u001b[38;5;28;01mfor\u001b[39;00m (key, value) \u001b[38;5;129;01min\u001b[39;00m index\u001b[38;5;241m.\u001b[39mitems()]) \n",
      "\u001b[1;31mNameError\u001b[0m: name 'imdb' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's decode the first review\n",
    "# Above you see the first review of the dataset which is labeled as positive (1).\n",
    "# The code below retrieves the dictionary mapping word indices back into the original words so that we can read them. \n",
    "# It replaces every unknown word with a “#”. It does this by using the get_word_index() function.\n",
    "\n",
    "\n",
    "# Retrieves a dict mapping words to their index in the IMDB dataset.\n",
    "index = imdb.get_word_index()\n",
    "# If there is a possibility of multiple keys with the same value, you will need to specify the desired behaviour in this case to lookup more than 2 values\n",
    "# ivd = dict((v, k) for k, v in d.items())\n",
    "# If you want to peek at the reviews yourself and see what people have actually written, you can reverse the process too:\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in data[0]] ) #The purpose of subtracting 3 from i is to adjust the indice,# to indicate that the index was not found.\n",
    "print(decoded) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aChnUXHEDAN",
    "outputId": "d82b016d-ac15-400d-8b94-19dc06e59e3c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m index\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YqMI4BpREJAC",
    "outputId": "04ff6012-d952-4909-9e80-488bd67e41d3"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reverse_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m reverse_index\n",
      "\u001b[1;31mNameError\u001b[0m: name 'reverse_index' is not defined"
     ]
    }
   ],
   "source": [
    "reverse_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "1bhmYzVhEHS6",
    "outputId": "f4ee3fde-e200-4d95-81e2-3456b4fe9695"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'decoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decoded\n",
      "\u001b[1;31mNameError\u001b[0m: name 'decoded' is not defined"
     ]
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Y7ACaI6ECaU8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Adding sequence to data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Vectorization is the process of converting textual data into numerical vectors and is a process that is usually applied once the text is cleaned.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m vectorize(data)\n\u001b[0;32m      4\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(label)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#Adding sequence to data\n",
    "# Vectorization is the process of converting textual data into numerical vectors and is a process that is usually applied once the text is cleaned.\n",
    "data = vectorize(data)\n",
    "label = np.array(label).astype(\"float32\")\n",
    "\n",
    "# Now it is time to prepare our data. We will vectorize every review and fill it with zeros so that it contains exactly 1000 numbers. \n",
    "# That means we fill every review that is shorter than 1000 with zeros. \n",
    "# We do this because the biggest review is nearly that long and every input for our neural network needs to have the same size. \n",
    "# We also transform the targets into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "75EaYgpSHmNz",
    "outputId": "78cc883e-8bbd-4b59-afbe-b511e72405e6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s710CMfaHpOa",
    "outputId": "efc4414f-2a11-4bef-b728-fe85602b7950"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m label\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "CoHZ4IPDDCjL"
   },
   "outputs": [],
   "source": [
    "# Let's check distribution of data\n",
    "\n",
    "# To create plots for EDA(exploratory data analysis) \n",
    "import seaborn as sns #seaborn is a popular Python visualization library that is built on top of Matplotlib and provides a high-level interface for creating informative and attractive statistical graphics.\n",
    "sns.set(color_codes=True)\n",
    "import matplotlib.pyplot as plt # %matplotlib to display Matplotlib plots inline with the notebook\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 303
    },
    "id": "WD3bOTy6DGO_",
    "outputId": "d8601ccb-0c08-4d3a-92b9-94ff63b12a07"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m labelDF\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:label})\n\u001b[0;32m      2\u001b[0m sns\u001b[38;5;241m.\u001b[39mcountplot(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, data\u001b[38;5;241m=\u001b[39mlabelDF)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "labelDF=pd.DataFrame({'label':label})\n",
    "sns.countplot(x='label', data=labelDF)\n",
    "\n",
    "# For below analysis it is clear that data has equel distribution of sentiments.This will help us building a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3VNm4V0DJ2h"
   },
   "outputs": [],
   "source": [
    "# Creating train and test data set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data,label, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXKlV64fEZSF",
    "outputId": "0ac2266a-d304-43a0-e699-bbc1d613c99a"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS032rQMEcYM",
    "outputId": "6eb47f85-e8ae-4a81-f58b-719037965471"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85VbeXfBEgca"
   },
   "outputs": [],
   "source": [
    "# Let's create sequential model,In deep learning, a Sequential model is a linear stack of layers, where you can simply add layers one after the other\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Phrv18kKEo2_",
    "outputId": "e1e472e2-8321-4c75-d3a5-28090112ccf5"
   },
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "# Note that we set the input-shape to 10,000 at the input-layer because our reviews are 10,000 integers long.\n",
    "# The input-layer takes 10,000 as input and outputs it with a shape of 50.\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
    "# Hidden - Layers\n",
    "# Please note you should always use a dropout rate between 20% and 50%. # here in our case 0.3 means 30% dropout we are using dropout to prevent overfitting. \n",
    "# By the way, if you want you can build a sentiment analysis without LSTMs(Long Short-Term Memory networks), then you simply need to replace it by a flatten layer:\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\")) #ReLU\" stands for Rectified Linear Unit, and it is a commonly used activation function in neural networks. \n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\")) #adds another Dense layer to the model, but with a single neuron instead of 50,i.e. out put layer,it produces the output predictions of the model.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXAKnIRaE7KH"
   },
   "outputs": [],
   "source": [
    "#For early stopping \n",
    "# Stop training when a monitored metric has stopped improving.\n",
    "# monitor: Quantity to be monitored.\n",
    "# patience: Number of epochs with no improvement after which training will be stopped.\n",
    "import tensorflow as tf #TensorFlow provides a wide range of tools and features for data processing, model building, model training, and model evaluation.\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NKv2l42xHKuN"
   },
   "outputs": [],
   "source": [
    "# We use the “adam” optimizer, an algorithm that changes the weights and biases during training.\n",
    "# During training, the weights and biases of a machine learning model are updated iteratively to minimize the difference between the model's predictions and the actual outputs.\n",
    "# We also choose binary-crossentropy as loss (because we deal with binary classification) and accuracy as our evaluation metric.\n",
    "\n",
    "model.compile(\n",
    " optimizer = \"adam\",\n",
    " loss = \"binary_crossentropy\",\n",
    " metrics = [\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RckYtYiNHOw8",
    "outputId": "d2757d25-539b-4eae-c0d8-76b74c175d36"
   },
   "outputs": [],
   "source": [
    "# Now we're able to train our model. We'll do this with a batch_size of 500 and only for two epochs because I recognized that the model overfits if we train it longer.\n",
    "# batch size defines the number of samples that will be propagated through the network.\n",
    "# For instance, let's say you have 1050 training samples and you want to set up a batch_size equal to 100. \n",
    "# The algorithm takes the first 100 samples (from 1st to 100th) from the training dataset and trains the network. \n",
    "# Next, it takes the second 100 samples (from 101st to 200th) and trains the network again. \n",
    "# We can keep doing this procedure until we have propagated all samples through of the network. \n",
    "# Problem might happen with the last set of samples. In our example, we've used 1050 which is not divisible by 100 without remainder.\n",
    "# The simplest solution is just to get the final 50 samples and train the network.\n",
    "##The goal is to find the number of epochs that results in good performance on a validation dataset without overfitting to the training data.\n",
    "results = model.fit(\n",
    " X_train, y_train,\n",
    " epochs= 2,\n",
    " batch_size = 500,\n",
    " validation_data = (X_test, y_test),\n",
    " callbacks=[callback]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94oaobMeHQ97",
    "outputId": "d5446ce6-d102-4594-dde9-db3f73c37840"
   },
   "outputs": [],
   "source": [
    "# Let's check mean accuracy of our model\n",
    "print(np.mean(results.history[\"val_accuracy\"])) # Good model should have a mean accuracy significantly higher than 50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "gXT3sRG8HYHo",
    "outputId": "cae57df6-9d19-478d-a4ad-38b6b8384014"
   },
   "outputs": [],
   "source": [
    "#Let's plot training history of our model\n",
    "\n",
    "# list all data in history\n",
    "print(results.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(results.history['accuracy']) #Plots the training accuracy of the model at each epoch.\n",
    "plt.plot(results.history['val_accuracy']) #Plots the validation accuracy of the model at each epoch.\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(results.history['loss'])\n",
    "plt.plot(results.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pdn0QM_DT_Xa",
    "outputId": "c07c10bc-bb22-4c42-e87a-c3af1e49e349"
   },
   "outputs": [],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLjG8bZoWX4P"
   },
   "outputs": [],
   "source": [
    "#Out put analysis,\n",
    "#[0.9865479] is a single prediction value for a particular input sample in the test data. \n",
    "#It is the predicted probability of the positive sentiment class (class 1) for that input.\n",
    "#Since the output activation function of the last layer of the model is sigmoid, which maps the predicted values to a range of [0,1]\n",
    "#,the output values represent the probabilities of the positive class. \n",
    "#In this case, the probability of the positive class for the given input sample is 0.9865479, which is very close to 1, indicating that the model is highly confident in predicting the positive class for that input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
